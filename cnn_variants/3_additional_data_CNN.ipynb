{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CS263_Final_Project.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Sarcasm Detection - CNN with extra Fields"
   ],
   "metadata": {
    "id": "mUvRpPVFl449"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation & Download"
   ],
   "metadata": {
    "collapsed": false,
    "id": "mEXU_qtY-dd6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install datasets\n",
    "    !pip install transformers\n",
    "    !pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "from sklearn import model_selection, feature_extraction\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric, Dataset\n",
    "from transformers import pipeline, AutoModel, BertTokenizer, BertModel, AutoConfig, AutoTokenizer, DataCollatorWithPadding\n",
    "import os\n",
    "from torch.nn.functional import pad\n",
    "from typing import List\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset as Ds"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wXK2dwt0-dd8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘key.csv’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘test-balanced.csv.bz2’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘train-balanced.csv.bz2’ already there; not retrieving.\r\n",
      "\r\n",
      "bzip2: Output file test-balanced.csv already exists.\r\n",
      "bzip2: Output file train-balanced.csv already exists.\r\n"
     ]
    }
   ],
   "source": [
    "# Source https://nlp.cs.princeton.edu/SARC/\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/key.csv'\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/main/test-balanced.csv.bz2'\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/main/train-balanced.csv.bz2'\n",
    "!bzip2 -dk 'test-balanced.csv.bz2'\n",
    "!bzip2 -dk 'train-balanced.csv.bz2'"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCPCXC7J-dd9",
    "outputId": "2d90b8eb-e69b-43cc-ca1a-c30c3a95c3ba"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data Set\n",
    "Citation: https://medium.com/@therpsvishal/sarcasm-detection-on-reddit-data-4b399df855ad"
   ],
   "metadata": {
    "id": "kPMqTcF2kc2u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Get Train Data\n",
    "header_names = pd.read_csv('key.csv', sep='\\t').columns.values.tolist()\n",
    "train = pd.read_csv('train-balanced.csv', sep='\\t', names=header_names)\n",
    "test = pd.read_csv('test-balanced.csv', sep='\\t', names=header_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "j9go4HWD-deA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments have lengths min:1.0, mean: 56.692298864334525, median: 46.0, max: 10000.0\n",
      "parent_comments have lengths min:1, mean: 133.35310923937453, median: 75.0, max: 40301\n",
      "\n",
      "Dropping 0.1% from min and max extremes we have:\n",
      "comments have lengths min:1.0, mean: 56.05465451131691, median: 46.0, max: 363.0\n",
      "parent_comments have lengths min:2.0, mean: 129.45715033182344, median: 75.0, max: 2579.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Parent Comments')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comment_lengths = train['comment'].str.len()\n",
    "parent_lengths = train['parent_comment'].str.len()\n",
    "print(\n",
    "    f'comments have lengths min:{comment_lengths.min()}, mean: {comment_lengths.mean()}, median: {comment_lengths.median()}, max: {comment_lengths.max()}')\n",
    "print(\n",
    "    f'parent_comments have lengths min:{parent_lengths.min()}, mean: {parent_lengths.mean()}, median: {parent_lengths.median()}, max: {parent_lengths.max()}')\n",
    "\n",
    "\n",
    "def drop_outliers(series, pct):\n",
    "    sorted_series = series.sort_values()\n",
    "    lower_pct, upper_pct = pct, 1 - pct\n",
    "    lower_idx, upper_idx = int(lower_pct * series.size), int(upper_pct * series.size)\n",
    "    lower_bound, upper_bound = sorted_series.tolist()[lower_idx], sorted_series.tolist()[upper_idx]\n",
    "    return series.where(lambda x: (lower_bound <= x) & (x <= upper_bound)).dropna()\n",
    "\n",
    "\n",
    "print('\\nDropping 0.1% from min and max extremes we have:')\n",
    "t_comment_lengths = drop_outliers(comment_lengths, 0.001)\n",
    "print(\n",
    "    f'comments have lengths min:{t_comment_lengths.min()}, mean: {t_comment_lengths.mean()}, median: {t_comment_lengths.median()}, max: {t_comment_lengths.max()}')\n",
    "\n",
    "t_parent_lengths = drop_outliers(parent_lengths, 0.001)\n",
    "print(\n",
    "    f'parent_comments have lengths min:{t_parent_lengths.min()}, mean: {t_parent_lengths.mean()}, median: {t_parent_lengths.median()}, max: {t_parent_lengths.max()}')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Distribution of comment lengths')\n",
    "ax1.hist(t_comment_lengths.tolist(), bins=1000)\n",
    "ax1.set_title('Comments')\n",
    "ax2.hist(t_parent_lengths.tolist(), bins=1000)\n",
    "ax2.set_title('Parent Comments')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SAXEEllg-deA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genuine score mean 6.097644773101247\n",
      "genuine score median 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genuine, sarcastic = train[train['label'] == 0], train[train['label'] == 1]\n",
    "genuine_score, genuine_ups, genuine_downs = genuine['score'], genuine['ups'], genuine['downs']\n",
    "\n",
    "# trim 0.1% from both ends\n",
    "t_genuine_score = drop_outliers(genuine_score, 0.001)\n",
    "t_genuine_ups = drop_outliers(genuine_ups, 0.001)\n",
    "t_genuine_downs = drop_outliers(genuine_downs, 0.001)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.suptitle('Distribution of scores for genuine comments')\n",
    "ax1.hist(t_genuine_score.tolist(), bins=50)\n",
    "ax1.set_title('Score')\n",
    "ax2.hist(t_genuine_ups.tolist(), bins=50)\n",
    "ax2.set_title('Ups')\n",
    "ax3.hist(t_genuine_downs.tolist(), bins=50)\n",
    "ax3.set_title('Downs')\n",
    "\n",
    "print(f'genuine score mean {t_genuine_score.mean()}')\n",
    "print(f'genuine score median {t_genuine_score.median()}')\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcastic score mean 5.642374285838894\n",
      "sarcastic score median 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarcastic_score, sarcastic_ups, sarcastic_downs = sarcastic['score'], sarcastic['ups'], sarcastic['downs']\n",
    "\n",
    "# trim 0.1% from both ends\n",
    "t_sarcastic_score = drop_outliers(sarcastic_score, 0.001)\n",
    "t_sarcastic_ups = drop_outliers(sarcastic_ups, 0.001)\n",
    "t_sarcastic_downs = drop_outliers(sarcastic_downs, 0.001)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.suptitle('Distribution of scores for sarcastic comments')\n",
    "ax1.hist(t_sarcastic_score.tolist(), bins=50)\n",
    "ax1.set_title('Score')\n",
    "ax2.hist(t_sarcastic_ups.tolist(), bins=50)\n",
    "ax2.set_title('Ups')\n",
    "ax3.hist(t_sarcastic_downs.tolist(), bins=50)\n",
    "ax3.set_title('Downs')\n",
    "\n",
    "print(f'sarcastic score mean {t_sarcastic_score.mean()}')\n",
    "print(f'sarcastic score median {t_sarcastic_score.median()}')\n",
    "fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "label_by_sub = train.groupby(['subreddit', 'label'], as_index=False).size()\n",
    "only_sarcastic_subs = label_by_sub[label_by_sub['label'] == 1]\n",
    "subs = train.groupby(['subreddit'], as_index=False).size()\n",
    "\n",
    "\n",
    "def get_sarcasm_rate(row):\n",
    "    name = row['subreddit']\n",
    "    sarcastic_entry = only_sarcastic_subs[only_sarcastic_subs['subreddit'] == name]\n",
    "    if sarcastic_entry.size > 0:\n",
    "        return sarcastic_entry.iloc[0]['size'] / row['size']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "sarcasm_rates_by_subreddit = subs.apply(get_sarcasm_rate, axis=1)\n",
    "#only_sarcastic[only_sarcastic['subreddit'] == 'a'].size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Rates of Sarcasm by Subreddit')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sarcasm_rates_by_subreddit.tolist(), bins=100)\n",
    "plt.xlabel('Rate of Sarcasm')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rates of Sarcasm by Subreddit')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "label_by_author = train.groupby(['author', 'label'], as_index=False).size()\n",
    "only_sarcastic_authors = label_by_author[label_by_author['label'] == 1]\n",
    "authors = train.groupby(['author'], as_index=False).size()\n",
    "\n",
    "\n",
    "def get_sarcasm_rate(row):\n",
    "    name = row['author']\n",
    "    sarcastic_entry = only_sarcastic_authors[only_sarcastic_authors['author'] == name]\n",
    "    if sarcastic_entry.size > 0:\n",
    "        return sarcastic_entry.iloc[0]['size'] / row['size']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Expensive to compute so only run when necessary.\n",
    "# sarcasm_rates_by_author = authors.apply(get_sarcasm_rate, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# plt.hist(sarcasm_rates_by_author.tolist(), bins=100)\n",
    "# plt.xlabel('Rate of Sarcasm')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Rates of Sarcasm by Author')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network (CNN)"
   ],
   "metadata": {
    "id": "Q3GazWXhpJod"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert-base-uncased']\n",
      "Saving experiment results to .\n"
     ]
    }
   ],
   "source": [
    "# CNN Code Here\n",
    "dirs = ['cnn', 'cnn/cache', 'cnn/runs', 'cnn/models']\n",
    "_, cache_dir, run_dir, model_dir = dirs\n",
    "seed = 777\n",
    "max_length = 512\n",
    "batch_size = 10\n",
    "data_percentage = 0.01  # Value in the range (0,1] to determine how much data to use for training.\n",
    "model_names = ['bert-base-uncased']\n",
    "model_output_size = 768  # depends on the embedding length generated by the encoder\n",
    "print(model_names)\n",
    "run_id = ''\n",
    "project_name = 'cs263-nlp-final'\n",
    "\n",
    "if IN_COLAB:\n",
    "    from requests import get\n",
    "\n",
    "    filename = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
    "    notebook_name = filename.split('.')[0]\n",
    "    save_path = f'/content/drive/MyDrive/Colab Notebooks/{notebook_name}-outputs'\n",
    "\n",
    "else:\n",
    "    save_path = '.'\n",
    "\n",
    "print(f'Saving experiment results to {save_path}')\n",
    "\n",
    "dirs = [save_path, f'{save_path}/cnn', f'{save_path}/cnn/cache', f'{save_path}/cnn/runs', f'{save_path}/cnn/models']\n",
    "_, _, cache_dir, run_dir, model_dir = dirs\n",
    "\n",
    "for d in dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from cnn_generator import create_cnn_model\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Neural Net Definitions\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self, input_shape: (int, int)):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape[0] * input_shape[1], 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "\n",
    "small_cnn_conf = {\n",
    "    'modules': [{\n",
    "        'type': 'conv',\n",
    "        'filter_count': 16,\n",
    "        'filter_size': (8, 8),\n",
    "        'pad': 0,\n",
    "        'stride': 4\n",
    "    },\n",
    "        {'type': 'pool',\n",
    "         'pool_size': (4, 4),\n",
    "         'pool_func': 'max',\n",
    "         'pad': 0,\n",
    "         'stride': 1\n",
    "         },\n",
    "        {\n",
    "            'type': 'conv',\n",
    "            'filter_count': 8,\n",
    "            'filter_size': (4, 4),\n",
    "            'pad': 0,\n",
    "            'stride': 2\n",
    "        }],\n",
    "    'fc_layers': [256],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, input_shape: (int, int)):\n",
    "        super().__init__()\n",
    "        #self.body = AutoModel.from_pretrained(model_name,config=AutoConfig.from_pretrained(model_name, output_attentions=True,output_hidden_states=True))\n",
    "        self.input_shape = input_shape\n",
    "        self.network = create_cnn_model(small_cnn_conf, (1,input_shape[0],input_shape[1]), classes=2)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # Need to add channel dimension for CNN\n",
    "        batch, height, width = xb.shape\n",
    "        return self.network(xb.view(batch,1,height,width))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SarcasmDataset(Ds):\n",
    "    def __init__(self, data: List[Tuple[str, int]]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # model = SmallCNN(\n",
    "    #     (max_length, model_output_size))\n",
    "    model = MultiCNN()\n",
    "    model.to('cuda:0')\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    # Uncomment for debugging or analysis purposes\n",
    "    print(model)\n",
    "    print(f'Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "    return model, loss_fn, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class MultiCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, (8, 8), stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d((4, 4), stride=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, (4, 4), stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Conv output + injected author and subreddit\n",
    "        self.dense1 = nn.Linear(45384 + 10*model_output_size*2, 256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x, extra_x):\n",
    "        batch, height, width = x.shape\n",
    "        # Need to add channel dimension for CNN\n",
    "        x_reshaped = x.view(batch, 1, height, width)\n",
    "        out = self.relu1(self.conv1(x_reshaped))\n",
    "        out = self.pool1(out)\n",
    "        out = self.relu2(self.conv2(out))\n",
    "        out = torch.concat((torch.flatten(out, start_dim=1),torch.flatten(extra_x, start_dim=1)),dim=1)\n",
    "        out = self.relu3(self.dense1(out))\n",
    "        out = self.output(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def train_one_epoch(epoch_index,\n",
    "                    tb_writer,\n",
    "                    training_loader,\n",
    "                    model,\n",
    "                    loss_fn,\n",
    "                    optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in tqdm.tqdm(enumerate(training_loader), total=len(training_loader)):\n",
    "        # Every data instance is an input + label pair\n",
    "        (comment_pairs, authors, subreddits), labels = data\n",
    "\n",
    "        # Zero your gradients for every bach\n",
    "        optimizer.zero_grad()\n",
    "        s = len(comment_pairs)\n",
    "        # Make predictions for this batch\n",
    "        embeddings = pipe(list(comment_pairs) + list(authors) + list(subreddits))\n",
    "        c_embeddings, a_embeddings, s_embeddings = embeddings[:s], embeddings[s:2 * s], embeddings[2 * s:]\n",
    "        tensor_y = labels.to('cuda:0')\n",
    "        tensors_x = [torch.tensor(co, device='cuda:0') for co in c_embeddings]\n",
    "\n",
    "        to_tensor = lambda l: torch.squeeze(torch.tensor(l, device='cuda:0'))\n",
    "        to_padded = lambda t: pad(t, (0, 0, 0, 10 - t.shape[0])) if t.shape[0] < 10 else t[:10,:]\n",
    "        tensors_xtra = [(to_tensor(au), to_tensor(su)) for au,su in zip(a_embeddings,s_embeddings)]\n",
    "        padded_xtra = torch.stack([torch.concat((to_padded(au),to_padded(su))) for au, su in tensors_xtra], 0)\n",
    "        padded_x = torch.squeeze(torch.stack([pad(t, (0, 0, 0, max_length - t.shape[1])) for t in tensors_x], 1), 0)\n",
    "        outputs = model(padded_x,padded_xtra)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, tensor_y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 0:\n",
    "            last_loss = running_loss / 200  # loss per batch\n",
    "            #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# We're not really using the pipeline optimally and it keeps complaining about that.\n",
    "warnings.simplefilter(\"ignore\")\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "def train_model(training_loader,\n",
    "                validation_loader,\n",
    "                model,\n",
    "                loss_fn,\n",
    "                optimizer,\n",
    "                fold):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter(f'{run_dir}/cnn_trainer_{timestamp}')\n",
    "    early_stopping_limit = 2  # how many epochs of decreased validation accuracy to tolerate until training is stopped early.\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "    best_accuracy = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'EPOCH {fold + 1}-{epoch + 1}')\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch, writer, training_loader, model, loss_fn, optimizer)\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        model.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        running_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in tqdm.tqdm(enumerate(validation_loader), total=len(validation_loader)):\n",
    "                (comment_pairs, authors, subreddits), labels = vdata\n",
    "                s = len(comment_pairs)\n",
    "                # Make predictions for this batch\n",
    "                embeddings = pipe(list(comment_pairs) + list(authors) + list(subreddits))\n",
    "                c_embeddings, a_embeddings, s_embeddings = embeddings[:s], embeddings[s:2 * s], embeddings[2 * s:]\n",
    "                tensor_y = labels.to('cuda:0')\n",
    "\n",
    "                tensors_x = [torch.tensor(co, device='cuda:0') for co in c_embeddings]\n",
    "                to_tensor = lambda l: torch.squeeze(torch.tensor(l, device='cuda:0'))\n",
    "\n",
    "                to_padded = lambda t: pad(t, (0, 0, 0, 10 - t.shape[0])) if t.shape[0] < 10 else t[:10,:]\n",
    "                tensors_xtra = [(to_tensor(au), to_tensor(su)) for au,su in zip(a_embeddings,s_embeddings)]\n",
    "                padded_xtra = torch.stack([torch.concat((to_padded(au),to_padded(su))) for au, su in tensors_xtra], 0)\n",
    "                padded_x = torch.squeeze(torch.stack([pad(t, (0, 0, 0, max_length - t.shape[1])) for t in tensors_x], 1), 0)\n",
    "                voutputs = model(padded_x,padded_xtra)\n",
    "\n",
    "                _, vpredictions = torch.max(voutputs, dim=1)\n",
    "                vloss = loss_fn(voutputs, tensor_y)\n",
    "                vpredictions = vpredictions.cpu()\n",
    "                tensor_y = tensor_y.cpu()\n",
    "                vaccuracy = accuracy_score(tensor_y, vpredictions)\n",
    "                vprecision = precision_score(tensor_y, vpredictions, zero_division=0)\n",
    "                vrecall = recall_score(tensor_y, vpredictions, zero_division=0)\n",
    "                vf1 = f1_score(tensor_y, vpredictions, zero_division=0)\n",
    "\n",
    "                running_vloss += vloss\n",
    "                running_accuracy += vaccuracy\n",
    "                running_precision += vprecision\n",
    "                running_recall += vrecall\n",
    "                running_f1 += vf1\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        avg_accuracy = running_accuracy / (i + 1)\n",
    "        avg_precision = running_precision / (i + 1)\n",
    "        avg_recall = running_recall / (i + 1)\n",
    "        avg_f1 = running_f1 / (i + 1)\n",
    "\n",
    "        print(\n",
    "            f'LOSS train {round(avg_loss, 3)} valid {round(avg_vloss.item(), 3)} valid accuracy {round(avg_accuracy, 3)}')\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                           {'Training': avg_loss,\n",
    "                            'Validation': avg_vloss, },\n",
    "                           epoch + 1)\n",
    "        writer.add_scalars('Metrics',\n",
    "                           {'Accuracy': avg_accuracy,\n",
    "                            'Precision': avg_precision,\n",
    "                            'Recall': avg_recall,\n",
    "                            'F1 Score': avg_f1, },\n",
    "                           epoch + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_vloss = avg_vloss\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_precision = avg_precision\n",
    "            best_recall = avg_recall\n",
    "            best_f1 = avg_f1\n",
    "\n",
    "            model_path = f'{model_dir}/{model.__class__.__name__}_{timestamp}_{epoch}'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        elif early_stopping_limit > 0:\n",
    "            early_stopping_limit -= 1\n",
    "        else:\n",
    "            print('Training stopped due to no improvement in accuracy.')\n",
    "            return best_accuracy, best_precision, best_recall, best_f1\n",
    "\n",
    "    return best_accuracy, best_precision, best_recall, best_f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10109it [00:00, 428887.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/11 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cc624b9205a4983a67049cab418e156"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOLD 1\n",
      "MultiCNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=(4, 4), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 8, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (dense1): Linear(in_features=60744, out_features=256, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "Trainable Parameters: 15554330\n",
      "EPOCH 1-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203/203 [01:15<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0 valid 0.694 valid accuracy 0.498\n",
      "EPOCH 1-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/203 [00:02<01:11,  2.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [42]\u001B[0m, in \u001B[0;36m<cell line: 66>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     77\u001B[0m     pipe \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature-extraction\u001B[39m\u001B[38;5;124m'\u001B[39m, model\u001B[38;5;241m=\u001B[39membedder, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     78\u001B[0m                     padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m'\u001B[39m, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     79\u001B[0m     feature \u001B[38;5;241m=\u001B[39m pipe(trainable_set[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomment_pair\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 80\u001B[0m     \u001B[43mk_fold_cross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtook \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(end \u001B[38;5;241m-\u001B[39m start, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [42]\u001B[0m, in \u001B[0;36mk_fold_cross_validation\u001B[0;34m(k, stop_after)\u001B[0m\n\u001B[1;32m     36\u001B[0m validation_loader \u001B[38;5;241m=\u001B[39m DataLoader(SarcasmDataset(validation_data), shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m     38\u001B[0m model, loss_function, optimizer \u001B[38;5;241m=\u001B[39m create_model()\n\u001B[0;32m---> 40\u001B[0m a, p, r, f \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining run metrics:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(a, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(p, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(r, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(f, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [41]\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(training_loader, validation_loader, model, loss_fn, optimizer, fold)\u001B[0m\n\u001B[1;32m     53\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(comment_pairs)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Make predictions for this batch\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcomment_pairs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mauthors\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubreddits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m c_embeddings, a_embeddings, s_embeddings \u001B[38;5;241m=\u001B[39m embeddings[:s], embeddings[s:\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m s], embeddings[\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m s:]\n\u001B[1;32m     57\u001B[0m tensor_y \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:79\u001B[0m, in \u001B[0;36mFeatureExtractionPipeline.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    Extract the features of the input(s).\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;03m        A nested list of `float`: The features computed by the model.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1015\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m can_use_iterator:\n\u001B[1;32m   1012\u001B[0m     final_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   1013\u001B[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[1;32m   1014\u001B[0m     )\n\u001B[0;32m-> 1015\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m [output \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m final_iterator]\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1015\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m can_use_iterator:\n\u001B[1;32m   1012\u001B[0m     final_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   1013\u001B[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[1;32m   1014\u001B[0m     )\n\u001B[0;32m-> 1015\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m [output \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m final_iterator]\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:111\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[1;32m    110\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m--> 111\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:111\u001B[0m, in \u001B[0;36mPipelineIterator.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader_batch_item()\n\u001B[1;32m    110\u001B[0m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[0;32m--> 111\u001B[0m item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfer(item, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:17\u001B[0m, in \u001B[0;36mPipelineDataset.__getitem__\u001B[0;34m(self, i)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, i):\n\u001B[1;32m     16\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[i]\n\u001B[0;32m---> 17\u001B[0m     processed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m processed\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:55\u001B[0m, in \u001B[0;36mFeatureExtractionPipeline.preprocess\u001B[0;34m(self, inputs, truncation)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtruncation\u001B[39m\u001B[38;5;124m\"\u001B[39m: truncation}\n\u001B[0;32m---> 55\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_inputs\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2497\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2477\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[1;32m   2478\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   2479\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2494\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2495\u001B[0m     )\n\u001B[1;32m   2496\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2497\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2500\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2513\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2515\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2516\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2570\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2560\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   2561\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   2562\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   2563\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2567\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2568\u001B[0m )\n\u001B[0;32m-> 2570\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2572\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2573\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2574\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2575\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2576\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2577\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2578\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2579\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2580\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2581\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2582\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2583\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2584\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2585\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2586\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2587\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2588\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2589\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:498\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_encode_plus\u001B[39m(\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    477\u001B[0m     text: Union[TextInput, PreTokenizedInput],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    494\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    495\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchEncoding:\n\u001B[1;32m    497\u001B[0m     batched_input \u001B[38;5;241m=\u001B[39m [(text, text_pair)] \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;28;01melse\u001B[39;00m [text]\n\u001B[0;32m--> 498\u001B[0m     batched_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatched_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    518\u001B[0m     \u001B[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001B[39;00m\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[39;00m\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_overflowing_tokens:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:425\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_truncation_and_padding(\n\u001B[1;32m    418\u001B[0m     padding_strategy\u001B[38;5;241m=\u001B[39mpadding_strategy,\n\u001B[1;32m    419\u001B[0m     truncation_strategy\u001B[38;5;241m=\u001B[39mtruncation_strategy,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    422\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m    423\u001B[0m )\n\u001B[0;32m--> 425\u001B[0m encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_pretokenized\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;66;03m# `Tokens` has type: Tuple[\u001B[39;00m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;66;03m#                       List[EncodingFast]\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[1;32m    437\u001B[0m tokens_and_encodings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_encoding(\n\u001B[1;32m    439\u001B[0m         encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[1;32m    449\u001B[0m ]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "r_mean = lambda v: round(np.mean(v), 3)\n",
    "r_min = lambda v: round(np.min(v), 3)\n",
    "r_max = lambda v: round(np.max(v), 3)\n",
    "r_med = lambda v: round(np.median(v), 3)\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(k=5, stop_after=None):\n",
    "    final_accuracies = []\n",
    "    final_precisions = []\n",
    "    final_recalls = []\n",
    "    final_f1s = []\n",
    "    splits_indices = [int(i / k * len(trainable_set)) for i in range(1, k + 1)]\n",
    "    splits_indices = [0] + splits_indices\n",
    "    folds_to_evaluate = k if stop_after is None else stop_after\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f'\\n\\nFOLD {i + 1}')\n",
    "        first, last = splits_indices[i], splits_indices[i + 1]\n",
    "        training_fold = (trainable_set['comment_pair'][:first] + trainable_set['comment_pair'][last:],\n",
    "                         trainable_set['author'][:first] + trainable_set['author'][last:],\n",
    "                         trainable_set['subreddit'][:first] + trainable_set['subreddit'][last:],\n",
    "                         trainable_set['label'][:first] + trainable_set['label'][last:])\n",
    "        validation_fold = (trainable_set['comment_pair'][first:last],\n",
    "                           trainable_set['author'][first:last],\n",
    "                           trainable_set['subreddit'][first:last],\n",
    "                           trainable_set['label'][first:last])\n",
    "        #print(f'Training fold indices: [{0},{first}) and [{last},{splits_indices[-1]}]')\n",
    "        #print(f'Validation fold indices: [{first},{last})')\n",
    "        training_data = [((comment_pair, author, subreddit), label) for comment_pair, author, subreddit, label in\n",
    "                         zip(training_fold[0], training_fold[1], training_fold[2], training_fold[3])]\n",
    "        validation_data = [((comment_pair, author, subreddit), label) for comment_pair, author, subreddit, label in\n",
    "                           zip(validation_fold[0], validation_fold[1], validation_fold[2], validation_fold[3])]\n",
    "        training_loader = DataLoader(SarcasmDataset(training_data), shuffle=True, batch_size=batch_size)\n",
    "        validation_loader = DataLoader(SarcasmDataset(validation_data), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        model, loss_function, optimizer = create_model()\n",
    "\n",
    "        a, p, r, f = train_model(training_loader, validation_loader, model, loss_function, optimizer, i)\n",
    "        print('Training run metrics:')\n",
    "        print(f'accuracy: {round(a, 3)}, precision: {round(p, 3)}, recall: {round(r, 3)}, F1: {round(f, 3)}')\n",
    "        final_accuracies.append(a)\n",
    "        final_precisions.append(p)\n",
    "        final_recalls.append(r)\n",
    "        final_f1s.append(f)\n",
    "\n",
    "        if i + 1 >= folds_to_evaluate:\n",
    "            break\n",
    "\n",
    "    print(f'---------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Average across all {k} runs:')\n",
    "    print(\n",
    "        f'accuracy: {r_mean(final_accuracies)}, precision: {r_mean(final_precisions)}, recall: {r_mean(final_recalls)}, F1: {r_mean(final_f1s)}')\n",
    "    print(f'\\nMin across all {k} runs:')\n",
    "    print(\n",
    "        f'accuracy: {r_min(final_accuracies)}, precision: {r_min(final_precisions)}, recall: {r_min(final_recalls)}, F1: {r_min(final_f1s)}')\n",
    "    print(f'\\nMax across all {k} runs:')\n",
    "    print(\n",
    "        f'accuracy: {r_max(final_accuracies)}, precision: {r_max(final_precisions)}, recall: {r_max(final_recalls)}, F1: {r_max(final_f1s)}')\n",
    "    print(f'\\nMedian across all {k} runs:')\n",
    "    print(\n",
    "        f'accuracy: {r_med(final_accuracies)}, precision: {r_med(final_precisions)}, recall: {r_med(final_recalls)}, F1: {r_med(final_f1s)}')\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    dataset = Dataset.from_pandas(train)\n",
    "    subset = dataset.train_test_split(test_size=data_percentage, seed=seed)['test']\n",
    "    combined = [f'{p_c}{tokenizer.sep_token}{c}' for p_c, c in\n",
    "                tqdm.tqdm(zip(subset['parent_comment'], subset['comment']))]\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    trainable_set = subset.add_column('comment_pair', combined)\n",
    "    #print(trainable_set[0])\n",
    "\n",
    "    embedder = AutoModel.from_pretrained(model_name)\n",
    "    pipe = pipeline('feature-extraction', model=embedder, tokenizer=tokenizer, max_length=512, truncation=True,\n",
    "                    padding='max_length', device=0)\n",
    "    feature = pipe(trainable_set['comment_pair'][0])\n",
    "    k_fold_cross_validation(k=5, stop_after=3)\n",
    "end = time.time()\n",
    "print(f'took {round(end - start, 2)}s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}