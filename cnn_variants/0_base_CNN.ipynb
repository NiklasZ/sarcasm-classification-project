{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CS263_Final_Project.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Sarcasm Detection - Base CNN"
   ],
   "metadata": {
    "id": "mUvRpPVFl449"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation & Download"
   ],
   "metadata": {
    "collapsed": false,
    "id": "mEXU_qtY-dd6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install datasets\n",
    "  !pip install transformers\n",
    "  !pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "from sklearn import model_selection, feature_extraction\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric, Dataset\n",
    "from transformers import pipeline, AutoModel, BertTokenizer, BertModel, AutoConfig, AutoTokenizer, DataCollatorWithPadding\n",
    "import os"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wXK2dwt0-dd8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘key.csv’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘test-balanced.csv.bz2’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘train-balanced.csv.bz2’ already there; not retrieving.\r\n",
      "\r\n",
      "bzip2: Output file test-balanced.csv already exists.\r\n",
      "bzip2: Output file train-balanced.csv already exists.\r\n"
     ]
    }
   ],
   "source": [
    "# Source https://nlp.cs.princeton.edu/SARC/\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/key.csv'\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/main/test-balanced.csv.bz2'\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/main/train-balanced.csv.bz2'\n",
    "!bzip2 -dk 'test-balanced.csv.bz2'\n",
    "!bzip2 -dk 'train-balanced.csv.bz2'"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCPCXC7J-dd9",
    "outputId": "2d90b8eb-e69b-43cc-ca1a-c30c3a95c3ba"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data Set\n",
    "Citation: https://medium.com/@therpsvishal/sarcasm-detection-on-reddit-data-4b399df855ad"
   ],
   "metadata": {
    "id": "kPMqTcF2kc2u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Get Train Data\n",
    "header_names = pd.read_csv('key.csv', sep='\\t').columns.values.tolist()\n",
    "train = pd.read_csv('train-balanced.csv', sep='\\t', names=header_names)\n",
    "test = pd.read_csv('test-balanced.csv', sep='\\t', names=header_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "j9go4HWD-deA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments have lengths min:1.0, mean: 56.692298864334525, median: 46.0, max: 10000.0\n",
      "parent_comments have lengths min:1, mean: 133.35310923937453, median: 75.0, max: 40301\n",
      "\n",
      "Dropping 0.1% from min and max extremes we have:\n",
      "comments have lengths min:1.0, mean: 56.05465451131691, median: 46.0, max: 363.0\n",
      "parent_comments have lengths min:1.0, mean: 93.54131855580702, median: 70.0, max: 363.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Parent Comments')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comment_lengths = train['comment'].str.len()\n",
    "parent_lengths = train['parent_comment'].str.len()\n",
    "print(\n",
    "    f'comments have lengths min:{comment_lengths.min()}, mean: {comment_lengths.mean()}, median: {comment_lengths.median()}, max: {comment_lengths.max()}')\n",
    "print(\n",
    "    f'parent_comments have lengths min:{parent_lengths.min()}, mean: {parent_lengths.mean()}, median: {parent_lengths.median()}, max: {parent_lengths.max()}')\n",
    "\n",
    "print('\\nDropping 0.1% from min and max extremes we have:')\n",
    "sorted_c = comment_lengths.sort_values()\n",
    "lower_idx, upper_idx = int(0.001 * comment_lengths.size), int(0.999 * comment_lengths.size)\n",
    "lower_bound, upper_bound = sorted_c.tolist()[lower_idx], sorted_c.tolist()[upper_idx]\n",
    "t_comment_lengths = comment_lengths.where(lambda x: (lower_bound <= x) & (x <= upper_bound)).dropna()\n",
    "print(\n",
    "    f'comments have lengths min:{t_comment_lengths.min()}, mean: {t_comment_lengths.mean()}, median: {t_comment_lengths.median()}, max: {t_comment_lengths.max()}')\n",
    "\n",
    "sorted_p = comment_lengths.sort_values()\n",
    "lower_idx, upper_idx = int(0.001 * parent_lengths.size), int(0.999 * parent_lengths.size)\n",
    "lower_bound, upper_bound = sorted_p.tolist()[lower_idx], sorted_p.tolist()[upper_idx]\n",
    "t_parent_lengths = parent_lengths.where(lambda x: (lower_bound <= x) & (x <= upper_bound)).dropna()\n",
    "print(\n",
    "    f'parent_comments have lengths min:{t_parent_lengths.min()}, mean: {t_parent_lengths.mean()}, median: {t_parent_lengths.median()}, max: {t_parent_lengths.max()}')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Distribution of comment lengths')\n",
    "ax1.hist(t_comment_lengths.tolist(), bins=1000)\n",
    "ax1.set_title('Comments')\n",
    "ax2.hist(t_parent_lengths.tolist(), bins=1000)\n",
    "ax2.set_title('Parent Comments')\n",
    "\n",
    "# train_df['comment_length'] = train_df['comment'].str.len()\n",
    "# train_df.hist(bins=100,column='comment_length')\n",
    "#comment_lengths.hist(bins=100)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SAXEEllg-deA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network (CNN)"
   ],
   "metadata": {
    "id": "Q3GazWXhpJod"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XSY/roberta-scarcasm-discriminator\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/327 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4810b34d4eb84592b34adf1b385fe327"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4fa6129834047c29fd1f9ae7b5c273f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1573238afbd14001818647a29b22ca92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving experiment results to .\n"
     ]
    }
   ],
   "source": [
    "# CNN Code Here\n",
    "dirs = ['cnn', 'cnn/cache', 'cnn/runs', 'cnn/models']\n",
    "_, cache_dir, run_dir, model_dir = dirs\n",
    "seed = 777\n",
    "max_length = 512\n",
    "batch_size = 10\n",
    "data_percentage = 0.01 # Value in the range (0,1] to determine how much data to use for training.\n",
    "model_name = 'bert-base-uncased'\n",
    "print(model_name)\n",
    "run_id = ''\n",
    "project_name = 'cs263-nlp-final'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if IN_COLAB:\n",
    "  from requests import get\n",
    "  filename = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
    "  notebook_name = filename.split('.')[0]\n",
    "  save_path = f'/content/drive/MyDrive/Colab Notebooks/{notebook_name}-outputs'\n",
    "\n",
    "else:\n",
    "  save_path = '.'\n",
    "\n",
    "print(f'Saving experiment results to {save_path}')\n",
    "\n",
    "dirs = [save_path, f'{save_path}/cnn', f'{save_path}/cnn/cache', f'{save_path}/cnn/runs', f'{save_path}/cnn/models']\n",
    "_, _, cache_dir, run_dir, model_dir = dirs\n",
    "\n",
    "for d in dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10109it [00:00, 1199440.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/11 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9522674934a7441880f2807142e56e19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'comment': 'It was secretly designed by the Japanese!', 'author': 'DemonicSquid', 'subreddit': 'Warthunder', 'score': 1, 'ups': 1, 'downs': 0, 'date': '2016-09', 'created_utc': 1473144798, 'parent_comment': 'Caernarvon repair cost Seriously gaijin , 12k SL repair ? Not to mention 400 per pop of APDS. And also IT-1 is only cost about 4k to repair , please raise the repair cost of IT-1 to prevent spam like how you prevent Caernarvon spam last time if you are not going to reduce caernarvon repair cost.', 'comment_pair': 'Caernarvon repair cost Seriously gaijin , 12k SL repair ? Not to mention 400 per pop of APDS. And also IT-1 is only cost about 4k to repair , please raise the repair cost of IT-1 to prevent spam like how you prevent Caernarvon spam last time if you are not going to reduce caernarvon repair cost.</s>It was secretly designed by the Japanese!'}\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(train)\n",
    "subset = dataset.train_test_split(test_size=data_percentage, seed=seed)['test']\n",
    "combined = [f'{p_c}{tokenizer.sep_token}{c}' for p_c, c in tqdm.tqdm(zip(subset['parent_comment'], subset['comment']))]\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainable_set = subset.add_column('comment_pair', combined)\n",
    "print(trainable_set[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/735 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eda63ab1ee674bedbbc2f274a314b0b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e12212329c2c4509aace1969bc23d39f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at XSY/roberta-scarcasm-discriminator were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at XSY/roberta-scarcasm-discriminator and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset as Ds\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "embedder = AutoModel.from_pretrained(model_name)\n",
    "pipe = pipeline('feature-extraction', model=embedder, tokenizer=tokenizer, max_length=512, truncation=True,\n",
    "                padding='max_length', device=0)\n",
    "feature = pipe(trainable_set['comment_pair'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from cnn_generator import create_cnn_model\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Neural Net Definitions\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self, input_shape: (int, int)):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape[0] * input_shape[1], 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "\n",
    "small_cnn_conf = {\n",
    "    'modules': [{\n",
    "        'type': 'conv',\n",
    "        'filter_count': 16,\n",
    "        'filter_size': (8, 8),\n",
    "        'pad': 0,\n",
    "        'stride':4\n",
    "    },\n",
    "    {   'type': 'pool',\n",
    "        'pool_size': (4, 4),\n",
    "        'pool_func': 'max',\n",
    "        'pad': 0,\n",
    "        'stride':1\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'filter_count': 8,\n",
    "        'filter_size': (4, 4),\n",
    "        'pad': 0,\n",
    "        'stride':2\n",
    "    }],\n",
    "    'fc_layers': [256],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, input_shape: (int, int)):\n",
    "        super().__init__()\n",
    "        #self.body = AutoModel.from_pretrained(model_name,config=AutoConfig.from_pretrained(model_name, output_attentions=True,output_hidden_states=True))\n",
    "        self.input_shape = input_shape\n",
    "        self.network = create_cnn_model(small_cnn_conf, (1,input_shape[0],input_shape[1]), classes=2)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # Need to add channel dimension for CNN\n",
    "        batch, height, width = xb.shape\n",
    "        return self.network(xb.view(batch,1,height,width))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def create_model():\n",
    "    model = SmallCNN((max_length, 768))  # The 768 length depends on the embedding size generated by the encoder.\n",
    "    model.to('cuda:0')\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    # Uncomment for debugging or analysis purposes\n",
    "    #print(model)\n",
    "    #print(f'Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "    return model, loss_fn, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index,\n",
    "                    tb_writer,\n",
    "                    training_loader,\n",
    "                    model,\n",
    "                    loss_fn,\n",
    "                    optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in tqdm.tqdm(enumerate(training_loader), total=len(training_loader)):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        embeddings = [pipe(i) for i in inputs]\n",
    "        tensor_y = labels.to('cuda:0')\n",
    "        tensors_x = [torch.tensor(e, device='cuda:0') for e in embeddings]\n",
    "        padded_x = torch.squeeze(torch.stack([pad(t, (0, 0, 0, max_length - t.shape[1])) for t in tensors_x], 1), 0)\n",
    "        #dev = padded_x.get_device()\n",
    "        outputs = model(padded_x)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, tensor_y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 0:\n",
    "            last_loss = running_loss / 200  # loss per batch\n",
    "            #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "# from datetime import datetime\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter(f'{run_dir}/cnn_trainer_{timestamp}')\n",
    "# train_one_epoch(1, writer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class SarcasmDataset(Ds):\n",
    "    def __init__(self, data: List[Tuple[str, int]]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# We're not really using the pipeline optimally and it keeps complaining about that.\n",
    "warnings.simplefilter(\"ignore\")\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_model(training_loader,\n",
    "                validation_loader,\n",
    "                model,\n",
    "                loss_fn,\n",
    "                optimizer,\n",
    "                fold):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter(f'{run_dir}/cnn_trainer_{timestamp}')\n",
    "    early_stopping_limit = 2 # how many epochs of decreased validation accuracy to tolerate until training is stopped early.\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "    best_accuracy = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'EPOCH {fold+1}-{epoch+1}')\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch, writer, training_loader, model, loss_fn, optimizer)\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        model.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        running_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in tqdm.tqdm(enumerate(validation_loader), total=len(validation_loader)):\n",
    "                vinputs, vlabels = vdata\n",
    "                vembeddings = [pipe(i) for i in vinputs]\n",
    "                tensor_y = vlabels.to('cuda:0')\n",
    "                tensors_x = [torch.tensor(e, device='cuda:0') for e in vembeddings]\n",
    "                padded_x = torch.squeeze(torch.stack([pad(t, (0, 0, 0, max_length - t.shape[1])) for t in tensors_x], 1), 0)\n",
    "\n",
    "                voutputs = model(padded_x)\n",
    "                _, vpredictions = torch.max(voutputs, dim=1)\n",
    "                vloss = loss_fn(voutputs, tensor_y)\n",
    "                vpredictions = vpredictions.cpu()\n",
    "                tensor_y = tensor_y.cpu()\n",
    "                vaccuracy = accuracy_score(tensor_y, vpredictions)\n",
    "                vprecision = precision_score(tensor_y, vpredictions, zero_division=0)\n",
    "                vrecall = recall_score(tensor_y, vpredictions, zero_division=0)\n",
    "                vf1 = f1_score(tensor_y, vpredictions, zero_division=0)\n",
    "\n",
    "                running_vloss += vloss\n",
    "                running_accuracy += vaccuracy\n",
    "                running_precision += vprecision\n",
    "                running_recall += vrecall\n",
    "                running_f1 += vf1\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        avg_accuracy = running_accuracy / (i + 1)\n",
    "        avg_precision = running_precision / (i + 1)\n",
    "        avg_recall = running_recall / (i + 1)\n",
    "        avg_f1 = running_f1 / (i + 1)\n",
    "\n",
    "        print(f'LOSS train {round(avg_loss,3)} valid {round(avg_vloss.item(),3)} valid accuracy {round(avg_accuracy,3)}')\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                           {'Training': avg_loss,\n",
    "                            'Validation': avg_vloss, },\n",
    "                           epoch + 1)\n",
    "        writer.add_scalars('Metrics',\n",
    "                           {'Accuracy': avg_accuracy,\n",
    "                            'Precision': avg_precision,\n",
    "                            'Recall': avg_recall,\n",
    "                            'F1 Score': avg_f1, },\n",
    "                           epoch + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_vloss = avg_vloss\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_precision = avg_precision\n",
    "            best_recall = avg_recall\n",
    "            best_f1 = avg_f1\n",
    "\n",
    "            model_path = f'{model_dir}/{model.__class__.__name__}_{timestamp}_{epoch}'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        elif early_stopping_limit > 0:\n",
    "            early_stopping_limit -= 1\n",
    "        else:\n",
    "            print('Training stopped due to no improvement in accuracy.')\n",
    "            return best_accuracy, best_precision, best_recall, best_f1\n",
    "\n",
    "\n",
    "    return best_accuracy, best_precision, best_recall, best_f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOLD 1\n",
      "EPOCH 1-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 209/809 [00:26<01:17,  7.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 49>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMedian across all \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m runs:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr_med(final_accuracies)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr_med(final_precisions)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr_med(final_recalls)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr_med(final_f1s)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 49\u001B[0m \u001B[43mk_fold_cross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtook \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(end \u001B[38;5;241m-\u001B[39m start, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36mk_fold_cross_validation\u001B[0;34m(k)\u001B[0m\n\u001B[1;32m     28\u001B[0m validation_loader \u001B[38;5;241m=\u001B[39m DataLoader(SarcasmDataset(validation_data), shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m     30\u001B[0m model, loss_function, optimizer \u001B[38;5;241m=\u001B[39m create_model()\n\u001B[0;32m---> 32\u001B[0m a, p, r, f \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining run metrics:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(a,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(p,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(r,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(f,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(training_loader, validation_loader, model, loss_fn, optimizer, fold)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001B[39;00m\n\u001B[1;32m     30\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 31\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# We don't need gradients on to do reporting\u001B[39;00m\n\u001B[1;32m     34\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(epoch_index, tb_writer, training_loader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Make predictions for this batch\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m [pipe(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m inputs]\n\u001B[1;32m     22\u001B[0m tensor_y \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     23\u001B[0m tensors_x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(e, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m embeddings]\n",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Make predictions for this batch\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m [\u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m inputs]\n\u001B[1;32m     22\u001B[0m tensor_y \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     23\u001B[0m tensors_x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(e, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m embeddings]\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:79\u001B[0m, in \u001B[0;36mFeatureExtractionPipeline.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    Extract the features of the input(s).\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;03m        A nested list of `float`: The features computed by the model.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1026\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1026\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1033\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1031\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1032\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1033\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1035\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:943\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m    941\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m    942\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 943\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    944\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    945\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:59\u001B[0m, in \u001B[0;36mFeatureExtractionPipeline._forward\u001B[0;34m(self, model_inputs)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_inputs):\n\u001B[0;32m---> 59\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_outputs\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:847\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    838\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    840\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    841\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    842\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    845\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    846\u001B[0m )\n\u001B[0;32m--> 847\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    860\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:523\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    514\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    515\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    516\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 523\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    533\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:409\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    399\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    406\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    408\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 409\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:336\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    326\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    328\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    334\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    335\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 336\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    346\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:200\u001B[0m, in \u001B[0;36mRobertaSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    192\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    198\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    199\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 200\u001B[0m     mixed_query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001B[39;00m\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001B[39;00m\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001B[39;00m\n\u001B[1;32m    205\u001B[0m     is_cross_attention \u001B[38;5;241m=\u001B[39m encoder_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "r_mean = lambda v: round(np.mean(v),3)\n",
    "r_min = lambda v: round(np.min(v),3)\n",
    "r_max = lambda v: round(np.max(v),3)\n",
    "r_med = lambda v: round(np.median(v),3)\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(k=5):\n",
    "    final_accuracies = []\n",
    "    final_precisions = []\n",
    "    final_recalls = []\n",
    "    final_f1s = []\n",
    "    splits_indices = [int(i / k * len(trainable_set)) for i in range(1, k + 1)]\n",
    "    splits_indices = [0] + splits_indices\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f'\\n\\nFOLD {i+1}')\n",
    "        first, last = splits_indices[i], splits_indices[i+1]\n",
    "        training_fold = (trainable_set['comment_pair'][:first] + trainable_set['comment_pair'][last:],\n",
    "                         trainable_set['label'][:first] + trainable_set['label'][last:])\n",
    "        validation_fold = (trainable_set['comment_pair'][first:last], trainable_set['label'][first:last])\n",
    "        #print(f'Training fold indices: [{0},{first}) and [{last},{splits_indices[-1]}]')\n",
    "        #print(f'Validation fold indices: [{first},{last})')\n",
    "        training_data = [(feature, label) for feature, label in zip(training_fold[0], training_fold[1])]\n",
    "        validation_data = [(feature, label) for feature, label in zip(validation_fold[0], validation_fold[1])]\n",
    "        training_loader = DataLoader(SarcasmDataset(training_data), shuffle=True, batch_size=batch_size)\n",
    "        validation_loader = DataLoader(SarcasmDataset(validation_data), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        model, loss_function, optimizer = create_model()\n",
    "\n",
    "        a, p, r, f = train_model(training_loader, validation_loader, model, loss_function, optimizer, i)\n",
    "        print('Training run metrics:')\n",
    "        print(f'accuracy: {round(a,3)}, precision: {round(p,3)}, recall: {round(r,3)}, F1: {round(f,3)}')\n",
    "        final_accuracies.append(a)\n",
    "        final_precisions.append(p)\n",
    "        final_recalls.append(r)\n",
    "        final_f1s.append(f)\n",
    "    print(f'---------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Average across all {k} runs:')\n",
    "    print(f'accuracy: {r_mean(final_accuracies)}, precision: {r_mean(final_precisions)}, recall: {r_mean(final_recalls)}, F1: {r_mean(final_f1s)}')\n",
    "    print(f'\\nMin across all {k} runs:')\n",
    "    print(f'accuracy: {r_min(final_accuracies)}, precision: {r_min(final_precisions)}, recall: {r_min(final_recalls)}, F1: {r_min(final_f1s)}')\n",
    "    print(f'\\nMax across all {k} runs:')\n",
    "    print(f'accuracy: {r_max(final_accuracies)}, precision: {r_max(final_precisions)}, recall: {r_max(final_recalls)}, F1: {r_max(final_f1s)}')\n",
    "    print(f'\\nMedian across all {k} runs:')\n",
    "    print(f'accuracy: {r_med(final_accuracies)}, precision: {r_med(final_precisions)}, recall: {r_med(final_recalls)}, F1: {r_med(final_f1s)}')\n",
    "\n",
    "k_fold_cross_validation()\n",
    "end = time.time()\n",
    "print(f'took {round(end - start, 2)}s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}