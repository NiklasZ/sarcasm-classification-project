{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CS263_Final_Project.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS263 Final Project\n",
    "## Sarcasm Detection"
   ],
   "metadata": {
    "id": "mUvRpPVFl449"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation & Download"
   ],
   "metadata": {
    "collapsed": false,
    "id": "mEXU_qtY-dd6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  !pip install datasets\n",
    "  !pip install transformers\n",
    "  !pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "from sklearn import model_selection, feature_extraction\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric, Dataset\n",
    "from transformers import pipeline, AutoModel, BertTokenizer, BertModel, AutoConfig, AutoTokenizer, DataCollatorWithPadding\n",
    "import os\n",
    "from torch.nn.functional import pad\n",
    "from typing import List\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset as Ds"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wXK2dwt0-dd8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘key.csv’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘test-balanced.csv.bz2’ already there; not retrieving.\r\n",
      "\r\n",
      "File ‘train-balanced.csv.bz2’ already there; not retrieving.\r\n",
      "\r\n",
      "bzip2: Output file test-balanced.csv already exists.\r\n",
      "bzip2: Output file train-balanced.csv already exists.\r\n"
     ]
    }
   ],
   "source": [
    "# Source https://nlp.cs.princeton.edu/SARC/\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/key.csv'\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/main/test-balanced.csv.bz2'\n",
    "!wget -nc 'https://nlp.cs.princeton.edu/SARC/0.0/main/train-balanced.csv.bz2'\n",
    "!bzip2 -dk 'test-balanced.csv.bz2'\n",
    "!bzip2 -dk 'train-balanced.csv.bz2'"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NCPCXC7J-dd9",
    "outputId": "2d90b8eb-e69b-43cc-ca1a-c30c3a95c3ba"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data Set\n",
    "Citation: https://medium.com/@therpsvishal/sarcasm-detection-on-reddit-data-4b399df855ad"
   ],
   "metadata": {
    "id": "kPMqTcF2kc2u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Get Train Data\n",
    "header_names = pd.read_csv('key.csv', sep='\\t').columns.values.tolist()\n",
    "train = pd.read_csv('train-balanced.csv', sep='\\t', names=header_names)\n",
    "test = pd.read_csv('test-balanced.csv', sep='\\t', names=header_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "j9go4HWD-deA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments have lengths min:1.0, mean: 56.692298864334525, median: 46.0, max: 10000.0\n",
      "parent_comments have lengths min:1, mean: 133.35310923937453, median: 75.0, max: 40301\n",
      "\n",
      "Dropping 0.1% from min and max extremes we have:\n",
      "comments have lengths min:1.0, mean: 56.05465451131691, median: 46.0, max: 363.0\n",
      "parent_comments have lengths min:2.0, mean: 129.45715033182344, median: 75.0, max: 2579.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Parent Comments')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comment_lengths = train['comment'].str.len()\n",
    "parent_lengths = train['parent_comment'].str.len()\n",
    "print(\n",
    "    f'comments have lengths min:{comment_lengths.min()}, mean: {comment_lengths.mean()}, median: {comment_lengths.median()}, max: {comment_lengths.max()}')\n",
    "print(\n",
    "    f'parent_comments have lengths min:{parent_lengths.min()}, mean: {parent_lengths.mean()}, median: {parent_lengths.median()}, max: {parent_lengths.max()}')\n",
    "\n",
    "\n",
    "def drop_outliers(series, pct):\n",
    "    sorted_series = series.sort_values()\n",
    "    lower_pct, upper_pct = pct, 1 - pct\n",
    "    lower_idx, upper_idx = int(lower_pct * series.size), int(upper_pct* series.size)\n",
    "    lower_bound, upper_bound = sorted_series.tolist()[lower_idx], sorted_series.tolist()[upper_idx]\n",
    "    return series.where(lambda x: (lower_bound <= x) & (x <= upper_bound)).dropna()\n",
    "\n",
    "\n",
    "print('\\nDropping 0.1% from min and max extremes we have:')\n",
    "# sorted_c = comment_lengths.sort_values()\n",
    "# lower_idx, upper_idx = int(0.001 * comment_lengths.size), int(0.999 * comment_lengths.size)\n",
    "# lower_bound, upper_bound = sorted_c.tolist()[lower_idx], sorted_c.tolist()[upper_idx]\n",
    "# t_comment_lengths = comment_lengths.where(lambda x: (lower_bound <= x) & (x <= upper_bound)).dropna()\n",
    "t_comment_lengths = drop_outliers(comment_lengths,0.001)\n",
    "print(\n",
    "    f'comments have lengths min:{t_comment_lengths.min()}, mean: {t_comment_lengths.mean()}, median: {t_comment_lengths.median()}, max: {t_comment_lengths.max()}')\n",
    "\n",
    "# sorted_p = comment_lengths.sort_values()\n",
    "# lower_idx, upper_idx = int(0.001 * parent_lengths.size), int(0.999 * parent_lengths.size)\n",
    "# lower_bound, upper_bound = sorted_p.tolist()[lower_idx], sorted_p.tolist()[upper_idx]\n",
    "# t_parent_lengths = parent_lengths.where(lambda x: (lower_bound <= x) & (x <= upper_bound)).dropna()\n",
    "t_parent_lengths = drop_outliers(parent_lengths, 0.001)\n",
    "print(\n",
    "    f'parent_comments have lengths min:{t_parent_lengths.min()}, mean: {t_parent_lengths.mean()}, median: {t_parent_lengths.median()}, max: {t_parent_lengths.max()}')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Distribution of comment lengths')\n",
    "ax1.hist(t_comment_lengths.tolist(), bins=1000)\n",
    "ax1.set_title('Comments')\n",
    "ax2.hist(t_parent_lengths.tolist(), bins=1000)\n",
    "ax2.set_title('Parent Comments')\n",
    "\n",
    "# train_df['comment_length'] = train_df['comment'].str.len()\n",
    "# train_df.hist(bins=100,column='comment_length')\n",
    "#comment_lengths.hist(bins=100)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SAXEEllg-deA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genuine score mean 6.097644773101247\n",
      "genuine score median 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genuine, sarcastic = train[train['label'] == 0], train[train['label'] == 1]\n",
    "genuine_score, genuine_ups, genuine_downs = genuine['score'], genuine['ups'], genuine['downs']\n",
    "\n",
    "# trim 0.1% from both ends\n",
    "t_genuine_score = drop_outliers(genuine_score, 0.001)\n",
    "t_genuine_ups = drop_outliers(genuine_ups, 0.001)\n",
    "t_genuine_downs = drop_outliers(genuine_downs, 0.001)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.suptitle('Distribution of scores for genuine comments')\n",
    "ax1.hist(t_genuine_score.tolist(), bins=10)\n",
    "ax1.set_title('Score')\n",
    "ax2.hist(t_genuine_ups.tolist(), bins=10)\n",
    "ax2.set_title('Ups')\n",
    "ax3.hist(t_genuine_downs.tolist(), bins=10)\n",
    "ax3.set_title('Downs')\n",
    "\n",
    "print(f'genuine score mean {t_genuine_score.mean()}')\n",
    "print(f'genuine score median {t_genuine_score.median()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcastic score mean 5.642374285838894\n",
      "sarcastic score median 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarcastic_score, sarcastic_ups, sarcastic_downs = sarcastic['score'], sarcastic['ups'], sarcastic['downs']\n",
    "\n",
    "# trim 0.1% from both ends\n",
    "t_sarcastic_score = drop_outliers(sarcastic_score, 0.001)\n",
    "t_sarcastic_ups = drop_outliers(sarcastic_ups, 0.001)\n",
    "t_sarcastic_downs = drop_outliers(sarcastic_downs, 0.001)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.suptitle('Distribution of scores for sarcastic comments')\n",
    "ax1.hist(t_sarcastic_score.tolist(), bins=10)\n",
    "ax1.set_title('Score')\n",
    "ax2.hist(t_sarcastic_ups.tolist(), bins=10)\n",
    "ax2.set_title('Ups')\n",
    "ax3.hist(t_sarcastic_downs.tolist(), bins=10)\n",
    "ax3.set_title('Downs')\n",
    "\n",
    "print(f'sarcastic score mean {t_sarcastic_score.mean()}')\n",
    "print(f'sarcastic score median {t_sarcastic_score.median()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "label_by_sub = train.groupby(['subreddit','label'], as_index=False).size()\n",
    "only_sarcastic_subs = label_by_sub[label_by_sub['label'] == 1]\n",
    "subs = train.groupby(['subreddit'],as_index=False).size()\n",
    "\n",
    "def get_sarcasm_rate(row):\n",
    "    name = row['subreddit']\n",
    "    sarcastic_entry = only_sarcastic_subs[only_sarcastic_subs['subreddit'] == name]\n",
    "    if sarcastic_entry.size > 0:\n",
    "        return sarcastic_entry.iloc[0]['size']/row['size']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sarcasm_rates_by_subreddit = subs.apply(get_sarcasm_rate, axis=1)\n",
    "#only_sarcastic[only_sarcastic['subreddit'] == 'a'].size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([5.883e+03, 2.000e+00, 2.000e+00, 3.000e+00, 4.000e+00, 1.500e+01,\n        4.000e+00, 1.000e+01, 1.600e+01, 2.000e+01, 2.100e+01, 3.300e+01,\n        6.500e+01, 9.000e+00, 7.700e+01, 1.600e+01, 1.100e+02, 2.000e+01,\n        4.300e+01, 1.100e+01, 1.960e+02, 3.400e+01, 5.800e+01, 3.700e+01,\n        1.600e+01, 3.720e+02, 3.800e+01, 5.800e+01, 1.180e+02, 4.400e+01,\n        1.030e+02, 5.900e+01, 3.700e+01, 5.960e+02, 5.300e+01, 6.900e+01,\n        9.400e+01, 1.200e+02, 1.110e+02, 4.100e+01, 2.600e+02, 9.800e+01,\n        1.710e+02, 7.800e+01, 1.240e+02, 9.900e+01, 1.200e+02, 1.080e+02,\n        8.300e+01, 5.400e+01, 1.288e+03, 7.300e+01, 1.100e+02, 8.700e+01,\n        8.500e+01, 1.150e+02, 9.000e+01, 9.900e+01, 5.100e+01, 3.400e+01,\n        1.930e+02, 3.700e+01, 4.700e+01, 3.300e+01, 2.600e+01, 1.900e+01,\n        3.400e+02, 1.300e+01, 8.000e+00, 2.500e+01, 8.000e+00, 3.200e+01,\n        7.000e+00, 1.100e+01, 0.000e+00, 1.050e+02, 7.000e+00, 5.000e+00,\n        6.000e+00, 0.000e+00, 3.600e+01, 4.000e+00, 1.000e+00, 1.600e+01,\n        1.000e+00, 6.000e+00, 0.000e+00, 3.000e+00, 0.000e+00, 0.000e+00,\n        1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n        0.000e+00, 1.000e+00, 0.000e+00, 2.042e+03]),\n array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n        0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n        0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n        0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n        0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n        0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n        0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n        0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n        0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n        0.99, 1.  ]),\n <BarContainer object of 100 artists>)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sarcasm_rates_by_subreddit.tolist(), bins=100)\n",
    "plt.xlabel('Rate of Sarcasm')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rates of Sarcasm by Subreddit')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [63]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 13\u001B[0m sarcasm_rates_by_author \u001B[38;5;241m=\u001B[39m \u001B[43mauthors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_sarcasm_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/frame.py:8839\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[1;32m   8828\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[1;32m   8830\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[1;32m   8831\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   8832\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   8837\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[1;32m   8838\u001B[0m )\n\u001B[0;32m-> 8839\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/apply.py:727\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[1;32m    725\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[0;32m--> 727\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/apply.py:851\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    850\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 851\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/apply.py:867\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    864\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    865\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[1;32m    866\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[0;32m--> 867\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    868\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[1;32m    869\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[1;32m    870\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[1;32m    871\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[0;32mIn [63]\u001B[0m, in \u001B[0;36mget_sarcasm_rate\u001B[0;34m(row)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_sarcasm_rate\u001B[39m(row):\n\u001B[1;32m      6\u001B[0m     name \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauthor\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m----> 7\u001B[0m     sarcastic_entry \u001B[38;5;241m=\u001B[39m only_sarcastic_authors[\u001B[43monly_sarcastic_authors\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mauthor\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m]\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sarcastic_entry\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m sarcastic_entry\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m/\u001B[39mrow[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/ops/common.py:70\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     66\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m     68\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/arraylike.py:40\u001B[0m, in \u001B[0;36mOpsMixin.__eq__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__eq__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__eq__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cmp_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meq\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/series.py:5623\u001B[0m, in \u001B[0;36mSeries._cmp_method\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m   5620\u001B[0m rvalues \u001B[38;5;241m=\u001B[39m extract_array(other, extract_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, extract_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   5622\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m np\u001B[38;5;241m.\u001B[39merrstate(\u001B[38;5;28mall\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 5623\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcomparison_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5625\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_result(res_values, name\u001B[38;5;241m=\u001B[39mres_name)\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:283\u001B[0m, in \u001B[0;36mcomparison_op\u001B[0;34m(left, right, op)\u001B[0m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_object_dtype(lvalues\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(rvalues, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 283\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mcomp_method_OBJECT_ARRAY\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    286\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:73\u001B[0m, in \u001B[0;36mcomp_method_OBJECT_ARRAY\u001B[0;34m(op, x, y)\u001B[0m\n\u001B[1;32m     71\u001B[0m     result \u001B[38;5;241m=\u001B[39m libops\u001B[38;5;241m.\u001B[39mvec_compare(x\u001B[38;5;241m.\u001B[39mravel(), y\u001B[38;5;241m.\u001B[39mravel(), op)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 73\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mlibops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscalar_compare\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mravel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "label_by_author = train.groupby(['author','label'], as_index=False).size()\n",
    "only_sarcastic_authors = label_by_author[label_by_author['label'] == 1]\n",
    "authors = train.groupby(['author'],as_index=False).size()\n",
    "\n",
    "def get_sarcasm_rate(row):\n",
    "    name = row['author']\n",
    "    sarcastic_entry = only_sarcastic_authors[only_sarcastic_authors['author'] == name]\n",
    "    if sarcastic_entry.size > 0:\n",
    "        return sarcastic_entry.iloc[0]['size']/row['size']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sarcasm_rates_by_author = authors.apply(get_sarcasm_rate, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Rates of Sarcasm by Author')"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sarcasm_rates_by_author.tolist(), bins=1000)\n",
    "plt.xlabel('Rate of Sarcasm')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rates of Sarcasm by Author')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network (CNN)"
   ],
   "metadata": {
    "id": "Q3GazWXhpJod"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['siebert/sentiment-roberta-large-english']\n",
      "Saving experiment results to .\n"
     ]
    }
   ],
   "source": [
    "# CNN Code Here\n",
    "dirs = ['cnn', 'cnn/cache', 'cnn/runs', 'cnn/models']\n",
    "_, cache_dir, run_dir, model_dir = dirs\n",
    "seed = 777\n",
    "max_length = 512\n",
    "batch_size = 10\n",
    "data_percentage = 0.01 # Value in the range (0,1] to determine how much data to use for training.\n",
    "model_names =  ['bert-base-uncased'] #['XSY/roberta-scarcasm-discriminator','XSY/albert-base-v2-scarcasm-discriminator','albert-base-v2','siebert/sentiment-roberta-large-english']\n",
    "model_output_size = 768 # or 1024, will vary by model\n",
    "print(model_names)\n",
    "run_id = ''\n",
    "project_name = 'cs263-nlp-final'\n",
    "\n",
    "if IN_COLAB:\n",
    "  from requests import get\n",
    "  filename = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
    "  notebook_name = filename.split('.')[0]\n",
    "  save_path = f'/content/drive/MyDrive/Colab Notebooks/{notebook_name}-outputs'\n",
    "\n",
    "else:\n",
    "  save_path = '.'\n",
    "\n",
    "print(f'Saving experiment results to {save_path}')\n",
    "\n",
    "dirs = [save_path, f'{save_path}/cnn', f'{save_path}/cnn/cache', f'{save_path}/cnn/runs', f'{save_path}/cnn/models']\n",
    "_, _, cache_dir, run_dir, model_dir = dirs\n",
    "\n",
    "for d in dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# dataset = Dataset.from_pandas(train)\n",
    "# subset = dataset.train_test_split(test_size=data_percentage, seed=seed)['test']\n",
    "# combined = [f'{p_c}{tokenizer.sep_token}{c}' for p_c, c in tqdm.tqdm(zip(subset['parent_comment'], subset['comment']))]\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# trainable_set = subset.add_column('comment_pair', combined)\n",
    "# print(trainable_set[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# from typing import List, Tuple\n",
    "# from torch.utils.data import TensorDataset, DataLoader, Dataset as Ds\n",
    "#\n",
    "# embedder = AutoModel.from_pretrained(model_name)\n",
    "# pipe = pipeline('feature-extraction', model=embedder, tokenizer=tokenizer, max_length=512, truncation=True,\n",
    "#                 padding='max_length', device=0)\n",
    "# feature = pipe(trainable_set['comment_pair'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from cnn_generator import create_cnn_model\n",
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Neural Net Definitions\n",
    "class BasicMLP(nn.Module):\n",
    "    def __init__(self, input_shape: (int, int)):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape[0] * input_shape[1], 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "\n",
    "small_cnn_conf = {\n",
    "    'modules': [{\n",
    "        'type': 'conv',\n",
    "        'filter_count': 16,\n",
    "        'filter_size': (8, 8),\n",
    "        'pad': 0,\n",
    "        'stride':4\n",
    "    },\n",
    "    {   'type': 'pool',\n",
    "        'pool_size': (4, 4),\n",
    "        'pool_func': 'max',\n",
    "        'pad': 0,\n",
    "        'stride':1\n",
    "    },\n",
    "    {\n",
    "        'type': 'conv',\n",
    "        'filter_count': 8,\n",
    "        'filter_size': (4, 4),\n",
    "        'pad': 0,\n",
    "        'stride':2\n",
    "    }],\n",
    "    'fc_layers': [256],\n",
    "    'batch_norm': False,\n",
    "    'dropout': 0,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, input_shape: (int, int)):\n",
    "        super().__init__()\n",
    "        #self.body = AutoModel.from_pretrained(model_name,config=AutoConfig.from_pretrained(model_name, output_attentions=True,output_hidden_states=True))\n",
    "        self.input_shape = input_shape\n",
    "        self.network = create_cnn_model(small_cnn_conf, (1,input_shape[0],input_shape[1]), classes=2)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # Need to add channel dimension for CNN\n",
    "        batch, height, width = xb.shape\n",
    "        return self.network(xb.view(batch,1,height,width))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class SarcasmDataset(Ds):\n",
    "    def __init__(self, data: List[Tuple[str, int]]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def create_model():\n",
    "    model = SmallCNN((max_length, model_output_size))  # The 768 length depends on the embedding size generated by the encoder.\n",
    "    model.to('cuda:0')\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    # Uncomment for debugging or analysis purposes\n",
    "    print(model)\n",
    "    print(f'Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "    return model, loss_fn, optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at siebert/sentiment-roberta-large-english were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at siebert/sentiment-roberta-large-english and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10109it [00:00, 1169499.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/11 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "815c94aaa11f4261a26394a0e9eba7dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-conv: (1, 512, 1024) --> [16, 127, 255]\n",
      "1-pool: [16, 127, 255] --> [16, 124, 252]\n",
      "2-conv: [16, 124, 252] --> [8, 61, 125]\n",
      "flatten: [8, 61, 125] --> 61000\n",
      "0-fc: 61000 --> 256\n",
      "output: 256 --> 2\n",
      "SmallCNN(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(4, 4), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 8, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "    (6): Linear(in_features=61000, out_features=256, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable Parameters: 15619866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 911/911 [03:10<00:00,  4.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6869312691688537"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_one_epoch(epoch_index,\n",
    "                    tb_writer,\n",
    "                    training_loader,\n",
    "                    model,\n",
    "                    loss_fn,\n",
    "                    optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in tqdm.tqdm(enumerate(training_loader), total=len(training_loader)):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        embeddings = pipe(list(inputs))\n",
    "        tensor_y = labels.to('cuda:0')\n",
    "        tensors_x = [torch.tensor(e, device='cuda:0') for e in embeddings]\n",
    "        padded_x = torch.squeeze(torch.stack([pad(t, (0, 0, 0, max_length - t.shape[1])) for t in tensors_x], 1), 0)\n",
    "        #dev = padded_x.get_device()\n",
    "        outputs = model(padded_x)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, tensor_y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 0:\n",
    "            last_loss = running_loss / 200  # loss per batch\n",
    "            #print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "# model_name = model_names[0]\n",
    "# embedder = AutoModel.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# pipe = pipeline('feature-extraction', model=embedder, tokenizer=tokenizer, max_length=512, truncation=True,\n",
    "#                     padding='max_length', device=0)\n",
    "# first, last = 0, 1000\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# dataset = Dataset.from_pandas(train)\n",
    "# subset = dataset.train_test_split(test_size=data_percentage, seed=seed)['test']\n",
    "# combined = [f'{p_c}{tokenizer.sep_token}{c}' for p_c, c in tqdm.tqdm(zip(subset['parent_comment'], subset['comment']))]\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# trainable_set = subset.add_column('comment_pair', combined)\n",
    "# training_fold = (trainable_set['comment_pair'][:first] + trainable_set['comment_pair'][last:],\n",
    "#                  trainable_set['label'][:first] + trainable_set['label'][last:])\n",
    "# validation_fold = (trainable_set['comment_pair'][first:last], trainable_set['label'][first:last])\n",
    "# #print(f'Training fold indices: [{0},{first}) and [{last},{splits_indices[-1]}]')\n",
    "# #print(f'Validation fold indices: [{first},{last})')\n",
    "# training_data = [(feature, label) for feature, label in zip(training_fold[0], training_fold[1])]\n",
    "# validation_data = [(feature, label) for feature, label in zip(validation_fold[0], validation_fold[1])]\n",
    "# training_loader = DataLoader(SarcasmDataset(training_data), shuffle=True, batch_size=batch_size)\n",
    "# from datetime import datetime\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# model, loss_function, optimizer = create_model()\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# writer = SummaryWriter(f'{run_dir}/cnn_trainer_{timestamp}')\n",
    "# train_one_epoch(1, writer, training_loader, model, loss_function, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# We're not really using the pipeline optimally and it keeps complaining about that.\n",
    "warnings.simplefilter(\"ignore\")\n",
    "EPOCHS = 10\n",
    "\n",
    "def train_model(training_loader,\n",
    "                validation_loader,\n",
    "                model,\n",
    "                loss_fn,\n",
    "                optimizer,\n",
    "                fold):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter(f'{run_dir}/cnn_trainer_{timestamp}')\n",
    "    early_stopping_limit = 2 # how many epochs of decreased validation accuracy to tolerate until training is stopped early.\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "    best_accuracy = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    best_f1 = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'EPOCH {fold+1}-{epoch+1}')\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(epoch, writer, training_loader, model, loss_fn, optimizer)\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        model.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        running_f1 = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in tqdm.tqdm(enumerate(validation_loader), total=len(validation_loader)):\n",
    "                vinputs, vlabels = vdata\n",
    "                vembeddings = [pipe(i) for i in vinputs]\n",
    "                tensor_y = vlabels.to('cuda:0')\n",
    "                tensors_x = [torch.tensor(e, device='cuda:0') for e in vembeddings]\n",
    "                padded_x = torch.squeeze(torch.stack([pad(t, (0, 0, 0, max_length - t.shape[1])) for t in tensors_x], 1), 0)\n",
    "\n",
    "                voutputs = model(padded_x)\n",
    "                _, vpredictions = torch.max(voutputs, dim=1)\n",
    "                vloss = loss_fn(voutputs, tensor_y)\n",
    "                vpredictions = vpredictions.cpu()\n",
    "                tensor_y = tensor_y.cpu()\n",
    "                vaccuracy = accuracy_score(tensor_y, vpredictions)\n",
    "                vprecision = precision_score(tensor_y, vpredictions, zero_division=0)\n",
    "                vrecall = recall_score(tensor_y, vpredictions, zero_division=0)\n",
    "                vf1 = f1_score(tensor_y, vpredictions, zero_division=0)\n",
    "\n",
    "                running_vloss += vloss\n",
    "                running_accuracy += vaccuracy\n",
    "                running_precision += vprecision\n",
    "                running_recall += vrecall\n",
    "                running_f1 += vf1\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        avg_accuracy = running_accuracy / (i + 1)\n",
    "        avg_precision = running_precision / (i + 1)\n",
    "        avg_recall = running_recall / (i + 1)\n",
    "        avg_f1 = running_f1 / (i + 1)\n",
    "\n",
    "        print(f'LOSS train {round(avg_loss,3)} valid {round(avg_vloss.item(),3)} valid accuracy {round(avg_accuracy,3)}')\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                           {'Training': avg_loss,\n",
    "                            'Validation': avg_vloss, },\n",
    "                           epoch + 1)\n",
    "        writer.add_scalars('Metrics',\n",
    "                           {'Accuracy': avg_accuracy,\n",
    "                            'Precision': avg_precision,\n",
    "                            'Recall': avg_recall,\n",
    "                            'F1 Score': avg_f1, },\n",
    "                           epoch + 1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_vloss = avg_vloss\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_precision = avg_precision\n",
    "            best_recall = avg_recall\n",
    "            best_f1 = avg_f1\n",
    "\n",
    "            model_path = f'{model_dir}/{model.__class__.__name__}_{timestamp}_{epoch}'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        elif early_stopping_limit > 0:\n",
    "            early_stopping_limit -= 1\n",
    "        else:\n",
    "            print('Training stopped due to no improvement in accuracy.')\n",
    "            return best_accuracy, best_precision, best_recall, best_f1\n",
    "\n",
    "\n",
    "    return best_accuracy, best_precision, best_recall, best_f1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10109it [00:00, 1235077.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/11 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5a9a2e51c524fbf9a2ce528fbcfc30b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at siebert/sentiment-roberta-large-english were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at siebert/sentiment-roberta-large-english and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FOLD 1\n",
      "0-conv: (1, 512, 1024) --> [16, 127, 255]\n",
      "1-pool: [16, 127, 255] --> [16, 124, 252]\n",
      "2-conv: [16, 124, 252] --> [8, 61, 125]\n",
      "flatten: [8, 61, 125] --> 61000\n",
      "0-fc: 61000 --> 256\n",
      "output: 256 --> 2\n",
      "SmallCNN(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(4, 4), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 8, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "    (6): Linear(in_features=61000, out_features=256, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable Parameters: 15619866\n",
      "EPOCH 1-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 70/809 [00:14<02:34,  4.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 54>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     64\u001B[0m     pipe \u001B[38;5;241m=\u001B[39m pipeline(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeature-extraction\u001B[39m\u001B[38;5;124m'\u001B[39m, model\u001B[38;5;241m=\u001B[39membedder, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     65\u001B[0m                     padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m'\u001B[39m, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     66\u001B[0m     feature \u001B[38;5;241m=\u001B[39m pipe(trainable_set[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcomment_pair\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 67\u001B[0m     \u001B[43mk_fold_cross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mstop_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtook \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(end \u001B[38;5;241m-\u001B[39m start, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36mk_fold_cross_validation\u001B[0;34m(k, stop_after)\u001B[0m\n\u001B[1;32m     28\u001B[0m validation_loader \u001B[38;5;241m=\u001B[39m DataLoader(SarcasmDataset(validation_data), shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m     30\u001B[0m model, loss_function, optimizer \u001B[38;5;241m=\u001B[39m create_model()\n\u001B[0;32m---> 32\u001B[0m a, p, r, f \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining run metrics:\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(a,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(p,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(r,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mround\u001B[39m(f,\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Input \u001B[0;32mIn [30]\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(training_loader, validation_loader, model, loss_fn, optimizer, fold)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001B[39;00m\n\u001B[1;32m     30\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 31\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# We don't need gradients on to do reporting\u001B[39;00m\n\u001B[1;32m     34\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[0;32mIn [29]\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(epoch_index, tb_writer, training_loader, model, loss_fn, optimizer)\u001B[0m\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Make predictions for this batch\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m [pipe(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m inputs]\n\u001B[1;32m     22\u001B[0m tensor_y \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     23\u001B[0m tensors_x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(e, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m embeddings]\n",
      "Input \u001B[0;32mIn [29]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Make predictions for this batch\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m [\u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m inputs]\n\u001B[1;32m     22\u001B[0m tensor_y \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     23\u001B[0m tensors_x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(e, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m embeddings]\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:79\u001B[0m, in \u001B[0;36mFeatureExtractionPipeline.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    Extract the features of the input(s).\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;03m        A nested list of `float`: The features computed by the model.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1026\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1026\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1033\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1031\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[1;32m   1032\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[0;32m-> 1033\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[1;32m   1035\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/base.py:943\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[0;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[1;32m    941\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[1;32m    942\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 943\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    944\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    945\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pipelines/feature_extraction.py:59\u001B[0m, in \u001B[0;36mFeatureExtractionPipeline._forward\u001B[0;34m(self, model_inputs)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_inputs):\n\u001B[0;32m---> 59\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_outputs\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:847\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    838\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    840\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    841\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    842\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    845\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    846\u001B[0m )\n\u001B[0;32m--> 847\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    860\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:523\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    514\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    515\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    516\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 523\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    533\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:450\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    447\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    448\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[0;32m--> 450\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[1;32m    455\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/pytorch_utils.py:241\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:463\u001B[0m, in \u001B[0;36mRobertaLayer.feed_forward_chunk\u001B[0;34m(self, attention_output)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[1;32m    462\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate(attention_output)\n\u001B[0;32m--> 463\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mintermediate_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:375\u001B[0m, in \u001B[0;36mRobertaOutput.forward\u001B[0;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor, input_tensor: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 375\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    376\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(hidden_states)\n\u001B[1;32m    377\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_states \u001B[38;5;241m+\u001B[39m input_tensor)\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ucla/natural_language_processing/final-project/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "r_mean = lambda v: round(np.mean(v),3)\n",
    "r_min = lambda v: round(np.min(v),3)\n",
    "r_max = lambda v: round(np.max(v),3)\n",
    "r_med = lambda v: round(np.median(v),3)\n",
    "\n",
    "def k_fold_cross_validation(k=5, stop_after=None):\n",
    "    final_accuracies = []\n",
    "    final_precisions = []\n",
    "    final_recalls = []\n",
    "    final_f1s = []\n",
    "    splits_indices = [int(i / k * len(trainable_set)) for i in range(1, k + 1)]\n",
    "    splits_indices = [0] + splits_indices\n",
    "    folds_to_evaluate = k if stop_after is None else stop_after\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f'\\n\\nFOLD {i+1}')\n",
    "        first, last = splits_indices[i], splits_indices[i+1]\n",
    "        training_fold = (trainable_set['comment_pair'][:first] + trainable_set['comment_pair'][last:],\n",
    "                         trainable_set['label'][:first] + trainable_set['label'][last:])\n",
    "        validation_fold = (trainable_set['comment_pair'][first:last], trainable_set['label'][first:last])\n",
    "        #print(f'Training fold indices: [{0},{first}) and [{last},{splits_indices[-1]}]')\n",
    "        #print(f'Validation fold indices: [{first},{last})')\n",
    "        training_data = [(feature, label) for feature, label in zip(training_fold[0], training_fold[1])]\n",
    "        validation_data = [(feature, label) for feature, label in zip(validation_fold[0], validation_fold[1])]\n",
    "        training_loader = DataLoader(SarcasmDataset(training_data), shuffle=True, batch_size=batch_size)\n",
    "        validation_loader = DataLoader(SarcasmDataset(validation_data), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        model, loss_function, optimizer = create_model()\n",
    "\n",
    "        a, p, r, f = train_model(training_loader, validation_loader, model, loss_function, optimizer, i)\n",
    "        print('Training run metrics:')\n",
    "        print(f'accuracy: {round(a,3)}, precision: {round(p,3)}, recall: {round(r,3)}, F1: {round(f,3)}')\n",
    "        final_accuracies.append(a)\n",
    "        final_precisions.append(p)\n",
    "        final_recalls.append(r)\n",
    "        final_f1s.append(f)\n",
    "\n",
    "        if i+1 >= folds_to_evaluate:\n",
    "            break\n",
    "\n",
    "    print(f'---------------------------------------------------------------------------------------------------------')\n",
    "    print(f'Average across all {k} runs:')\n",
    "    print(f'accuracy: {r_mean(final_accuracies)}, precision: {r_mean(final_precisions)}, recall: {r_mean(final_recalls)}, F1: {r_mean(final_f1s)}')\n",
    "    print(f'\\nMin across all {k} runs:')\n",
    "    print(f'accuracy: {r_min(final_accuracies)}, precision: {r_min(final_precisions)}, recall: {r_min(final_recalls)}, F1: {r_min(final_f1s)}')\n",
    "    print(f'\\nMax across all {k} runs:')\n",
    "    print(f'accuracy: {r_max(final_accuracies)}, precision: {r_max(final_precisions)}, recall: {r_max(final_recalls)}, F1: {r_max(final_f1s)}')\n",
    "    print(f'\\nMedian across all {k} runs:')\n",
    "    print(f'accuracy: {r_med(final_accuracies)}, precision: {r_med(final_precisions)}, recall: {r_med(final_recalls)}, F1: {r_med(final_f1s)}')\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    dataset = Dataset.from_pandas(train)\n",
    "    subset = dataset.train_test_split(test_size=data_percentage, seed=seed)['test']\n",
    "    combined = [f'{p_c}{tokenizer.sep_token}{c}' for p_c, c in tqdm.tqdm(zip(subset['parent_comment'], subset['comment']))]\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    trainable_set = subset.add_column('comment_pair', combined)\n",
    "    #print(trainable_set[0])\n",
    "\n",
    "    embedder = AutoModel.from_pretrained(model_name)\n",
    "    pipe = pipeline('feature-extraction', model=embedder, tokenizer=tokenizer, max_length=512, truncation=True,\n",
    "                    padding='max_length', device=0)\n",
    "    feature = pipe(trainable_set['comment_pair'][0])\n",
    "    k_fold_cross_validation(k=5,stop_after=3)\n",
    "end = time.time()\n",
    "print(f'took {round(end - start, 2)}s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "KcIGz7YP-deC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# F1 Scoring Function using a Count Vectorizer\n",
    "# Does 5-fold train+validation\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score),\n",
    "           'recall': make_scorer(recall_score),\n",
    "           'f1_score': make_scorer(f1_score)}\n",
    "\n",
    "\n",
    "def getScores(clf, train_vectors, train):\n",
    "    # Get Scores\n",
    "    scores = model_selection.cross_validate(clf, train_vectors, train[\"label\"], cv=5, scoring=scoring)\n",
    "    return scores"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NNZBsR7s-deD"
   }
  }
 ]
}